{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRqNIPPX+ohnXVsalSxAhl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rodzeymer/Imersao_Python/blob/main/Imers%C3%A3o_Python_Aula_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta Aula 03, vamos trabalhar puramente com Python e para isso precisamos estruturar nossas ideias e os códigos, pois eles devem seguir uma cadeia lógica de instruções, de forma que quando executarmos todas as intruções a direção da execução seja linear e direta, ou seja, começa em um ponto e termina em outro sem fazer curvas e loops, e aqui vamos fazer de cima para baixo, ou como é conhecido Drop-down.\n",
        "\n",
        "Para facilitar a leitura irei comentar dentro dos blocos de código usando a cerquilha, conhecido como HashTag."
      ],
      "metadata": {
        "id": "DrNkl8bLVrR9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "fHR4_YYqD-m_"
      },
      "outputs": [],
      "source": [
        "# Nesse primeiro bloco que deixei em separado vão as importações de bibliotecas,\n",
        "#sempre que uma biblioteca nova precisar ser adicionada ela será adicionada aqui.\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Começando o tratamento dos dados precisamos primeiro carregar o arquivo a ser\n",
        "# trabalhado ou que servirá como base de dados para as análises, então adicionamos\n",
        "# ele no Notebook e carregamos ele pelo Pandas, fazendo sua leitura.\n",
        "# Podemos atribuir cada Planilha a uma variável, facilitando sua manipulação\n",
        "\n",
        "caminhoArquivo = \"/content/Planilha auxiliar - Imersão Python - Tabela de ações.xlsx\"\n",
        "\n",
        "df_principal = pd.read_excel(caminhoArquivo, sheet_name=\"Principal\")\n",
        "df_total_acoes = pd.read_excel(caminhoArquivo, sheet_name=\"Total_de_acoes\")\n",
        "df_ticker = pd.read_excel(caminhoArquivo, sheet_name=\"Ticker\")\n",
        "df_ChatGPT = pd.read_excel(caminhoArquivo, sheet_name=\"Chatgpt\")"
      ],
      "metadata": {
        "id": "IY2ibhAPEqO3"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui começamos a montar nossa tabela principal, podemos também criar uma nova planilha,\n",
        "# mas como já temos uma com vários dados que precisamos vamos sobreescrevê-la\n",
        "# com os dados que nos interessa\n",
        "\n",
        "df_principal = df_principal[[\"Ativo\", \"Data\", \"Último (R$)\", \"Var. Dia (%)\"]].copy()\n"
      ],
      "metadata": {
        "id": "XY-ZTU9_FbRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# À medida que iremos adicionando novas colunas ao nosso dataframe, que é um conjunto de vetores de dados,\n",
        "# vamos renomeando os títulos que não estiverem atendendo a padrões a a convenções\n",
        "# de boas práticas de programação, evitando que os códigos quebrem devido a espaços vazios\n",
        "# e a acentos e caracteres especiais, que dependendo da linguagem pode não suportá-los.\n",
        "\n",
        "df_principal = df_principal.rename(columns={\"Último (R$)\":\"Valor_Final\", \"Var. Dia (%)\":\"Var_Dia_Pct\"}).copy()"
      ],
      "metadata": {
        "id": "WKQx_btpFpdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui começamos a criar novas colunas, como por exemplo a variação em percentual, já que a coluna \"Var_Dia_Pct\"\n",
        "# apresenta a variação em decimal, a coluna \"Var_Pct\" agora mostra o valor em percentual, agora podemos\n",
        "# calcular o valor inicial, colocando os valores devidos na coluna \"Valor_Inicial\n",
        "\n",
        "df_principal['Var_Pct'] = df_principal[\"Var_Dia_Pct\"] / 100\n",
        "df_principal[\"Valor_Inicial\"] = df_principal[\"Valor_Final\"] / (df_principal['Var_Pct'] + 1)"
      ],
      "metadata": {
        "id": "JS2vZh9GG0iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Além de criar novas colunas, podemos pegar colunas de outras tabelas, lembrando que temos que\n",
        "# carregalas através do Pandas para termos acesso a elas.\n",
        "\n",
        "# Aqui faremos o Merge, ou junção, do dataframe \"df_total_acoes\", é importante que as tabelas\n",
        "# tenham uma coluna em comum, a chamada chave estrangeira, foreing key (FK), para que os dados\n",
        "# fiquem organizados. Veja que indicamos como chave estrangeira no \"df_principal\" a coluna \"Ativo\",\n",
        "# que corresponde à coluna \"Código\" do \"df_total_acoes\", e o sentido de inclusão foi da direita (right)\n",
        "# para a esquerda, logo a \"df_principal\" serve de base e ganha as colunas do \"df_total_acoes\"\n",
        "\n",
        "df_principal = df_principal.merge(df_total_acoes, left_on=\"Ativo\", right_on=\"Código\", how=\"left\")\n"
      ],
      "metadata": {
        "id": "YbU-L3e9Hubl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora observe que o df_principal possui duas colunas de iguais, as colunas Ativo\n",
        "# e Código, precisamos fazer a exclusão de uma dessas, como a Código está à direita da tabela,\n",
        "# se misturando aos dados numéricos faremos o drop dela, a função Drop descarta a coluna indicada,\n",
        "# como se vê abaixo.\n",
        "\n",
        "df_principal = df_principal.drop(columns=[\"Código\"])"
      ],
      "metadata": {
        "id": "1nzSuMCLIj3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Temos agora a coluna de Qtdr Teórica, que indica uma quantidade teórica de ações\n",
        "# disponíveis na data da aquisição dos dados, com o Valor_Final e Valor_Inicial\n",
        "# podemos calcular a sua variação em reais e calcular o valor da variação de cada ação\n",
        "\n",
        "df_principal[\"Variacao_RS\"] = (df_principal[\"Valor_Final\"] - df_principal[\"Valor_Inicial\"]) * df_principal['Qtde. Teórica']"
      ],
      "metadata": {
        "id": "IBYpgV1DI-e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como nos dados do tipo float estamos tratando de dados que só nos importam até\n",
        "# a segunda casa decimal vamos formatar esses números para apresentar apenas\n",
        "# essas duas casas decimais, alterando uma opção global do Pandas.\n",
        "\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ],
      "metadata": {
        "id": "68b31UuWJ9cj"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vejamos que a coluna Qtde_Teórica estaá sendo tratada como do tipo float, como\n",
        "# não existe fração de ação vamos trocar esse tipo para inteiro, descartando assim\n",
        "# as casas decimais da coluna, sem alterar os seus valores.\n",
        "\n",
        "df_principal[\"Qtde. Teórica\"] = df_principal[\"Qtde. Teórica\"].astype(int)"
      ],
      "metadata": {
        "id": "PM6J559cNq0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precisamos renomear o título dessa coluna também\n",
        "\n",
        "df_principal = df_principal.rename(columns={\"Qtde. Teórica\":\"Qtde_Teorica\"}).copy()"
      ],
      "metadata": {
        "id": "sY3KQcQ9OEbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora vamos começar nossas análises, primeiramente vamos separar as ações\n",
        "# de quem subiu, ficou estável e desceu, usando uma fórmula simples.\n",
        "\n",
        "# O comando .apply permite executar uma função em todos os elementos de uma\n",
        "# coluna especificada, nesse caso criamos a coluna Resultado, atribuindo a ela a\n",
        "# coluna Variacao_RS e aplicando a essa coluna a seguinte função.\n",
        "# Chamamos lambda para informar que os dados da coluna Variacao_RS serão\n",
        "# submetidos à seguinte # fórmula, o resultado será \"Subiu\", se X (a Variacao_RS no caso)\n",
        "# for maior que 0 # ou será \"Desceu\" se X for menor que 0,\n",
        "# se a Variacao_RS não subiu (X>0) e nem desceu (X<0) então o resultado será \"Estável\"\n",
        "\n",
        "df_principal[\"Resultado\"] = df_principal[\"Variacao_RS\"].apply(lambda x: \"Subiu\" if x >0 else(\"Desceu\" if x <0 else \"Estável\"))"
      ],
      "metadata": {
        "id": "a1R4eLQWOuAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neste passo iremos fazer mais um merge, agora com o df_ticker, pegando as colunas\n",
        "# Ativo do df_principal e Ticker do df_ticker como chaves em comum, e puxando as\n",
        "# colunas de df_ticker para df_principal, e já incluímos o drop da coluna Ticker.\n",
        "\n",
        "# vemos que a cada merge fazemos um drop, pois, em geral, as duas colunas de\n",
        "# correspondência possuem os mesmos dados, por isso elas são usadas como critério de ordenação\n",
        "\n",
        "df_principal = df_principal.merge(df_ticker, left_on=\"Ativo\", right_on=\"Ticker\", how=\"left\")\n",
        "df_principal = df_principal.drop(columns=[\"Ticker\"])\n"
      ],
      "metadata": {
        "id": "zGaa7Vo2PQZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mais um merge e mais um drop, agora pegamos as colunas do df_Chatgpt, pareando as\n",
        "# colunas Nome e Nome da empresa, do df_principal e df_Chatgpt, respectivamente,\n",
        "# fazendo o drop da coluna Nome da empresa ao final.\n",
        "\n",
        "df_principal = df_principal.merge(df_ChatGPT, left_on=\"Nome\", right_on=\"Nome da empresa\", how=\"left\")\n",
        "df_principal = df_principal.drop(columns=[\"Nome da empresa\"])"
      ],
      "metadata": {
        "id": "ganvjK1YQckq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seguindo as boas boas práticas de programação faremos a renomeação do título da\n",
        "# coluna Idade (anos) para Idade apenas.\n",
        "\n",
        "df_principal = df_principal.rename(columns={\"Idade (anos)\":\"Idade\"}).copy()"
      ],
      "metadata": {
        "id": "lX8d8xUeS9_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora mais uma análise interessante, vamos categorizar as empresas pela sua idade\n",
        "# veja que usamos mais uma vez a função lamdba, para aplicar a nossa fórmula em\n",
        "# toda a coluna Idade, guardando os resultados em Cat_Idade e com isso podemos\n",
        "# verificar quantas empresas estão em cada catergoria e quanto de variação cada\n",
        "# categoria agregou nesse dia.\n",
        "\n",
        "df_principal[\"Cat_Idade\"] = df_principal[\"Idade\"].apply(lambda x: 'Mais de 100' if x>100 else ('Menos de 50' if x<50 else 'Entre 50 e 100'))\n"
      ],
      "metadata": {
        "id": "cMTOYXpPY518"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E nessa parte vamos verificar qual foi a ação que mais subiu e qual mais desceu\n",
        "# além de ver a média da variação, e podemos ainda ver qual a média de subida e\n",
        "# a média da descida\n",
        "\n",
        "maior = df_principal['Variacao_RS'].max()\n",
        "menor = df_principal['Variacao_RS'].min()\n",
        "media = df_principal ['Variacao_RS'].mean()\n",
        "media_subiu = df_principal[df_principal[\"Resultado\"] == \"Subiu\"][\"Variacao_RS\"].mean()\n",
        "media_desceu = df_principal[df_principal[\"Resultado\"] == \"Desceu\"][\"Variacao_RS\"].mean()\n",
        "\n",
        "# Imprimindo Resultados\n",
        "print(f\"Maior\\t R$ {maior:,.2f}\")\n",
        "print(f\"Menor\\t R$ {menor:,.2f}\")\n",
        "print(f\"Média\\t R$ {media:,.2f}\")\n",
        "print(f\"Média de quem subiu\\t R$ {media_subiu:,.2f}\")\n",
        "print(f\"Média de quem desceu\\t R$ {media_desceu:,.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMTEuyLKcMIv",
        "outputId": "d877faba-78bc-4245-8205-96ccf4800dcd"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maior\t R$ 4,762,926,995.25\n",
            "Menor\t R$ -1,807,432,634.46\n",
            "Média\t R$ 218,008,898.33\n",
            "Média de quem subiu\t R$ 538,720,638.36\n",
            "Média de quem desceu\t R$ -186,265,310.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos analisar as ações que subiram e colocar elas em um dataframe\n",
        "# separado, como fizemos nesse df_principal_subiu\n",
        "\n",
        "df_principal_subiu = df_principal[df_principal[\"Resultado\"] == \"Subiu\"]"
      ],
      "metadata": {
        "id": "wUoofWnGEXQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E por fim podemos plotar gráficos das análises, nessa figura 1 colocamos em\n",
        "# barras a variação por resultado, ou seja o total de variação positiva e o total\n",
        "# de variação negativa.\n",
        "\n",
        "figura_1 = px.bar(df_analise_saldo, x='Resultado', y='Variacao_RS', text='Variacao_RS',\n",
        "                  title=\"Variação Reais por Resultado\", color=\"Resultado\", height=500, width=800)\n",
        "\n",
        "figura_1.update_yaxes(tickprefix=\"R$ \")\n",
        "figura_1.update_traces(texttemplate=\"R$ %{text:,.2f}\")\n",
        "figura_1.add_hline(y=0, line_color='black', line_dash=\"dot\")\n",
        "\n",
        "figura_1.show()"
      ],
      "metadata": {
        "id": "AMa0m_SNFweE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos fazer uma análise por segmentos, verificando a variação final de cada\n",
        "# segmento e sua participação na variação geral das ações.\n",
        "figura_2 = px.pie(df_analise_segmento, values='Variacao_RS', names='Segmento', title='Variação Reais por Segmento', height=500, width=800)\n",
        "figura_2.update_traces(textposition='inside', textinfo='percent+label', hovertemplate=\"Variação: R$ %{value:,.2f}\")\n",
        "\n",
        "figura_2.show()\n"
      ],
      "metadata": {
        "id": "Y8qF8N6zIJyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# E analisar finalmente a variação final por categoria de idade, podendo extrair\n",
        "# informações interessantes, já que se que as empresas jovens subiram mais que as\n",
        "# centenárias, mas que as empresas maduras subiram bem mais, podendo essa variação\n",
        "# exarcebada ser devido a uma verdadeira valorização das suas ações o pela quantidade\n",
        "# de empresas maduras que constam nesse banco de dados.\n",
        "\n",
        "df_analise_idade = df_principal.groupby(\"Cat_Idade\").agg({\"Variacao_RS\": \"sum\"}).reset_index()\n",
        "df_analise_idade = df_analise_idade.sort_values(by=\"Cat_Idade\", key=lambda x: x.map({\"Menos de 50\": 0, \"Entre 50 e 100\": 1, \"Mais de 100\": 2}))\n",
        "\n",
        "figura_3 = px.bar(df_analise_idade, x='Cat_Idade', y='Variacao_RS', text='Variacao_RS',\n",
        "                  title=\"Variação Reais por Categoria de Idade\", color=\"Cat_Idade\", height=500, width=800)\n",
        "\n",
        "figura_3.update_yaxes(tickprefix=\"R$ \")\n",
        "figura_3.update_traces(texttemplate=\"R$ %{text:,.2f}\")\n",
        "figura_3.add_hline(y=0, line_color='black', line_dash=\"dot\")\n",
        "\n",
        "figura_3.show()\n"
      ],
      "metadata": {
        "id": "6x7TFKYuVZGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pro fim, foi demonstrado que uma análise de dados, apesar de menos visual a princípio, é mais eficiente e profunda ao se utilizar ferramentas em Python, suas bibliotecas permitem manipulações cada vez mais detalhadas e com extração de informações interessantes para as tomadas de decisão.\n",
        "\n",
        "Lembrando que essa é uma exploração dos dados, que por não se ter uma pergunta bem definida não extrai o máximo desse banco de dados, e o próprio BD perde seu valor ao longo tempo, já que é um recorte de um dia na bolsa, pode ter maior valor agregado se anaálisado junto a uma série temporal maior, ou com mais informações do negócio a ser tratado.\n",
        "\n",
        "Nos vemos na próxima aula.!"
      ],
      "metadata": {
        "id": "LdT7N0vHqgkx"
      }
    }
  ]
}